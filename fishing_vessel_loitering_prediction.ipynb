{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Fishing Vessel Extened Loitering Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import ast\n",
    "import requests\n",
    "from pandas import json_normalize\n",
    "from geopy.distance import geodesic"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Loading\n",
    "Data Loading and Processing Workflow:\n",
    "1. Load and clean loitering data\n",
    "2. Create loitering-event-related features using loitering and anchorage data. (loiterting is our main dataset)\n",
    "3. At this point, we should have our feature added loitering dataset\n",
    "4. Then use each unique combination of timestamp and lat/lon (can be general area of port), to pull relevant weather data\n",
    "5. Join weather data back to loitering dataset using the same timestamp and location variables\n",
    "### Data loading & Initial Cleaning - Loitering and Ports Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def flatten_json_in_column(df, column_name):\n",
    "    \"\"\"\n",
    "    Flatten JSON strings in a column into separate columns.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input DataFrame\n",
    "    column_name (str): Name of column containing JSON strings\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with flattened JSON columns\n",
    "    \"\"\"\n",
    "    # Convert JSON strings to dictionaries\n",
    "    df[column_name] = df[column_name].apply(json.loads)\n",
    "\n",
    "    # Flatten the JSON column\n",
    "    flattened = pd.json_normalize(df[column_name])\n",
    "\n",
    "    # Add prefix to avoid column name conflicts\n",
    "    flattened.columns = [f\"{column_name}_{col}\" for col in flattened.columns]\n",
    "\n",
    "    # Drop the original JSON column and join with flattened data\n",
    "    df = df.drop(columns=[column_name]).join(flattened)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_vessel_events(df):\n",
    "    \"\"\"\n",
    "    Process vessel events dataframe by flattening nested JSON data.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input DataFrame with vessel events\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: Processed DataFrame with flattened columns\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataframe to avoid modifying the original\n",
    "    df = df.copy()\n",
    "\n",
    "    # Convert event_vessels from string to list of dictionaries\n",
    "    df['event_vessels'] = df['event_vessels'].apply(json.loads)\n",
    "\n",
    "    # Extract the first vessel's information (assuming one vessel per event)\n",
    "    vessel_info = pd.json_normalize([vessels[0] for vessels in df['event_vessels']])\n",
    "\n",
    "    # Rename vessel_info columns to avoid conflicts\n",
    "    rename_dict = {\n",
    "        'id': 'vessel_info_id',\n",
    "        'ssvid': 'vessel_info_ssvid',\n",
    "        'name': 'vessel_info_name',\n",
    "        'flag': 'vessel_info_flag'\n",
    "    }\n",
    "    vessel_info = vessel_info.rename(columns=rename_dict)\n",
    "\n",
    "    # Reset index for both dataframes to ensure proper joining\n",
    "    df = df.reset_index(drop=True)\n",
    "    vessel_info = vessel_info.reset_index(drop=True)\n",
    "\n",
    "    # Drop the original event_vessels column and join with vessel info\n",
    "    df = df.drop(columns=['event_vessels'])\n",
    "    df = pd.concat([df, vessel_info], axis=1)\n",
    "\n",
    "    # Process event_info column\n",
    "    df = flatten_json_in_column(df, 'event_info')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_and_process_data(loitering_path):\n",
    "    \"\"\"\n",
    "    Load and process both loitering dataset.\n",
    "    \"\"\"\n",
    "    # Read CSV file\n",
    "    loitering_df = pd.read_csv(loitering_path)\n",
    "\n",
    "    # Process event_vessels column (a nested JSON column)\n",
    "    processed_loitering_df = process_vessel_events(loitering_df)\n",
    "\n",
    "    # Convert datetime columns\n",
    "    datetime_cols = ['event_start', 'event_end']\n",
    "    for df in [processed_loitering_df]:\n",
    "        for col in datetime_cols:\n",
    "            df[col] = pd.to_datetime(df[col])\n",
    "\n",
    "    # Convert numeric columns in loitering dataset\n",
    "    numeric_cols = {\n",
    "        'event_info_median_speed_knots': float,\n",
    "        'event_info_total_distance_km': float,\n",
    "        'event_info_loitering_hours': float\n",
    "    }\n",
    "\n",
    "    for col, dtype in numeric_cols.items():\n",
    "        processed_loitering_df[col] = pd.to_numeric(processed_loitering_df[col], errors='raise')\n",
    "\n",
    "    # Remove rows with missing values\n",
    "    processed_loitering_df = processed_loitering_df.dropna()\n",
    "\n",
    "    # Drop irrelevant columns\n",
    "    processed_loitering_df = drop_irrelevant_columns(processed_loitering_df)\n",
    "\n",
    "    return processed_loitering_df\n",
    "\n",
    "\n",
    "def drop_irrelevant_columns(loitering_df):\n",
    "    \"\"\"\n",
    "    Drop irrelevant or redundant columns from dataset\n",
    "    - Only need lat_mean and lon_mean instead of their min and max\n",
    "    - vessel_info: only need vessel id and flag\n",
    "    - event_info_:\n",
    "        - event_info_elevation_m seems irrelevant\n",
    "        - origin_port: only need an ID for each port, using port_id, which is iso and label combined with a dash\n",
    "        - destination_port: only need an ID for each port, using port_id, which is iso and label combined with a dash\n",
    "        - regions: doesn't provide meaningful data, as its in codes\n",
    "    \"\"\"\n",
    "    # can probably drop event_info_origin_port.label and event_info_destination_port.label as well\n",
    "    loitering_cols_to_drop = [\n",
    "        'lat_min', 'lat_max', 'lon_min', 'lon_max',\n",
    "        'vessel_info_id', 'vessel_info_ssvid', 'vessel_info_name', 'event_geography', 'event_info_elevation_m',\n",
    "        'event_info_origin_port.anchorage_id', 'event_info_destination_port.anchorage_id',\n",
    "        'event_info_regions.eez', 'event_info_regions.fao', 'event_info_regions.rfmo'\n",
    "    ]\n",
    "\n",
    "    loitering_df = loitering_df.drop(columns=loitering_cols_to_drop)\n",
    "\n",
    "    return loitering_df\n",
    "\n",
    "loitering_path = 'data/CVP_loitering_202301.csv'\n",
    "\n",
    "loitering_df = load_and_process_data(loitering_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data loading & Initial Cleaning - Anchorage Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define dtype mapping for loading the CSV\n",
    "dtype_mapping = {\n",
    "    's2id': 'str',\n",
    "    'lat': 'float',\n",
    "    'lon': 'float',\n",
    "    'label': 'str',\n",
    "    'sublabel': 'str',\n",
    "    'label_source': 'str',\n",
    "    'iso3': 'str',\n",
    "    'distance_from_shore_m': 'float',\n",
    "    'drift_radius': 'float',\n",
    "    'dock': 'object'\n",
    "}\n",
    "\n",
    "# Load CSV with explicit types\n",
    "anchorage_df = pd.read_csv('data/named_anchorages_v2_20221206.csv', dtype=dtype_mapping)\n",
    "\n",
    "# Filter to keep only rows where dock is True\n",
    "anchorage_df['dock'] = anchorage_df['dock'].fillna(False).astype(bool)\n",
    "anchorage_df = anchorage_df[anchorage_df['dock'] == True]\n",
    "\n",
    "# Function to clean strings by removing whitespace and converting to lowercase\n",
    "def clean_string(x):\n",
    "    if pd.isna(x):\n",
    "        return ''\n",
    "    return str(x).replace(' ', '').lower()\n",
    "\n",
    "# Clean the iso3 and label columns\n",
    "anchorage_df['iso3_clean'] = anchorage_df['iso3'].apply(clean_string)\n",
    "anchorage_df['label_clean'] = anchorage_df['label'].apply(clean_string)\n",
    "\n",
    "# Create port_id column by combining cleaned iso3 and label\n",
    "anchorage_df['port_id'] = anchorage_df['iso3_clean'] + '-' + anchorage_df['label_clean']\n",
    "\n",
    "# Group by port_id and calculate mean lat and lon\n",
    "port_locations_df = anchorage_df.groupby('port_id').agg({\n",
    "    'lat': 'mean',\n",
    "    'lon': 'mean',\n",
    "    's2id': 'nunique'  # Count unique anchorage points\n",
    "}).reset_index()\n",
    "\n",
    "# Rename s2id count column to port_capacity\n",
    "port_locations_df = port_locations_df.rename(columns={'s2id': 'port_capacity'})\n",
    "\n",
    "# Round the coordinates to 6 decimal places for reasonable precision\n",
    "port_locations_df['lat'] = port_locations_df['lat'].round(6)\n",
    "port_locations_df['lon'] = port_locations_df['lon'].round(6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initial Feature Engineering - Loiterting and Anchorage Dataset\n",
    "Port Location Features:\n",
    "- Mapped origin and destination port locations (lat/lon)\n",
    "- Added port capacity for both origin and destination\n",
    "\n",
    "Distance Features (using geopy):\n",
    "- Port-to-port distance (origin to destination)\n",
    "\n",
    "Temporal Features:\n",
    "- Start day of week, month, hour\n",
    "- Time since last loitering event\n",
    "\n",
    "Loitering Classification:\n",
    "- Target variable: extended_loitering (>24 hours)\n",
    "- Location type: port_loiter (<50km from shore) vs deepsea_loiter\n",
    "- Loitering transition patterns (e.g., port_to_deepsea)\n",
    "\n",
    "Historical Statistics per Vessel:\n",
    "- Frequencies over different windows (30D, 90D, 365D):\n",
    "- Port-adjacent loitering count\n",
    "- Deep-sea loitering count\n",
    "\n",
    "Average durations by type:\n",
    "- Port-adjacent loitering\n",
    "- Deep-sea loitering\n",
    "\n",
    "- Speed variability (std dev of median_speed_knots)\n",
    "\n",
    "Port Congestion:\n",
    "- Daily count of port-adjacent loitering vessels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def add_port_location_features(loitering_df, port_locations_df):\n",
    "    \"\"\"\n",
    "    Add origin and destination port location features to loitering dataset\n",
    "    \"\"\"\n",
    "    # Create copy to avoid modifying original\n",
    "    df = loitering_df.copy()\n",
    "\n",
    "    # Join with origin port locations\n",
    "    origin_ports = port_locations_df.copy()\n",
    "    origin_ports.columns = [f'origin_port_{col}' for col in origin_ports.columns]\n",
    "    df = df.merge(\n",
    "        origin_ports,\n",
    "        left_on='event_info_origin_port.port_id',\n",
    "        right_on='origin_port_port_id',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Join with destination port locations\n",
    "    dest_ports = port_locations_df.copy()\n",
    "    dest_ports.columns = [f'destination_port_{col}' for col in dest_ports.columns]\n",
    "    df = df.merge(\n",
    "        dest_ports,\n",
    "        left_on='event_info_destination_port.port_id',\n",
    "        right_on='destination_port_port_id',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_target_variable(df):\n",
    "    \"\"\"Create binary target variable for extended loitering events\"\"\"\n",
    "    df['extended_loitering'] = (df['event_info_loitering_hours'] > 24).astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def classify_loitering_location(df):\n",
    "    \"\"\"Classify loitering events as port-adjacent (1) or deep-sea (0) based on shore distance\"\"\"\n",
    "    df['loitering_type'] = np.where(df['event_info_distance_from_shore_m'] < 50000, 1, 0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_distances(df):\n",
    "    \"\"\"Calculate various distance features using geopy\"\"\"\n",
    "    # Create copy to avoid modification warnings\n",
    "    df = df.copy()\n",
    "\n",
    "    # Remove rows with missing port locations\n",
    "    df = df.dropna(subset=['origin_port_lat', 'origin_port_lon', 'destination_port_lat', 'destination_port_lon'])\n",
    "\n",
    "    # Calculate distances using .loc to avoid warnings\n",
    "    df.loc[:, 'origin_destination_distance'] = df.apply(\n",
    "        lambda row: geodesic(\n",
    "            (row['origin_port_lat'], row['origin_port_lon']),\n",
    "            (row['destination_port_lat'], row['destination_port_lon'])\n",
    "        ).kilometers, axis=1\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_temporal_features(df):\n",
    "    \"\"\"Extract temporal features from event timestamps\"\"\"\n",
    "    # Event start features\n",
    "    df['start_dayofweek'] = df['event_start'].dt.dayofweek\n",
    "    df['start_month'] = df['event_start'].dt.month\n",
    "    df['start_hour'] = df['event_start'].dt.hour\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_historical_features(df):\n",
    "    \"\"\"Calculate historical statistics for each vessel\"\"\"\n",
    "    # Create copy and reset index to ensure proper alignment\n",
    "    df = df.copy().reset_index(drop=True)\n",
    "\n",
    "    # Sort by vessel and time\n",
    "    df = df.sort_values(['vessel_id', 'event_start'])\n",
    "\n",
    "    # Time since last loitering event\n",
    "    df['time_since_last_loiter'] = df.groupby('vessel_id')['event_start'].diff()\n",
    "\n",
    "    # Loitering transition types\n",
    "    df['prev_loitering_type'] = df.groupby('vessel_id')['loitering_type'].shift()\n",
    "\n",
    "    # Create binary columns for each loitering type\n",
    "    df['is_port_loiter'] = (df['loitering_type'] == 1).astype(int)\n",
    "    df['is_deepsea_loiter'] = (df['loitering_type'] == 0).astype(int)\n",
    "\n",
    "    # Historical frequencies\n",
    "    for window in ['30D', '90D', '365D']:\n",
    "        # Port-adjacent frequency\n",
    "        df[f'port_loiter_freq_{window}'] = (\n",
    "            df.set_index('event_start')\n",
    "            .groupby('vessel_id')['is_port_loiter']\n",
    "            .rolling(window)\n",
    "            .sum()\n",
    "            .reset_index()['is_port_loiter']\n",
    "        )\n",
    "\n",
    "        # Deep-sea frequency\n",
    "        df[f'deepsea_loiter_freq_{window}'] = (\n",
    "            df.set_index('event_start')\n",
    "            .groupby('vessel_id')['is_deepsea_loiter']\n",
    "            .rolling(window)\n",
    "            .sum()\n",
    "            .reset_index()['is_deepsea_loiter']\n",
    "        )\n",
    "\n",
    "    # Drop temporary columns\n",
    "    df = df.drop(['is_port_loiter', 'is_deepsea_loiter', 'prev_loitering_type'], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_averages_and_stats(df):\n",
    "    \"\"\"Calculate vessel-level averages and statistics\"\"\"\n",
    "    # Average distance between ports\n",
    "    df['avg_distance_travelled'] = df.groupby('vessel_id')['origin_destination_distance'].transform('mean')\n",
    "\n",
    "    # Average loitering durations by type\n",
    "    df['avg_port_loiter_duration'] = (\n",
    "        df[df['loitering_type'] == 1]\n",
    "        .groupby('vessel_id')['event_info_loitering_hours']\n",
    "        .transform('mean')\n",
    "    )\n",
    "\n",
    "    df['avg_deepsea_loiter_duration'] = (\n",
    "        df[df['loitering_type'] == 'deepsea']\n",
    "        .groupby('vessel_id')['event_info_loitering_hours']\n",
    "        .transform('mean')\n",
    "    )\n",
    "\n",
    "    # Speed variability\n",
    "    df['speed_variability'] = (\n",
    "        df.groupby('vessel_id')['event_info_median_speed_knots']\n",
    "        .transform('std')\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_port_congestion(df):\n",
    "    \"\"\"Calculate daily port congestion\"\"\"\n",
    "    df['event_date'] = df['event_start'].dt.date\n",
    "\n",
    "    port_congestion = (\n",
    "        df[df['loitering_type'] == 'port']\n",
    "        .groupby(['event_date'])\n",
    "        .size()\n",
    "        .reset_index(name='port_congestion')\n",
    "    )\n",
    "\n",
    "    df = df.merge(port_congestion, on='event_date', how='left')\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_features(loitering_df, port_locations_df):\n",
    "    \"\"\"Main function to process all feature engineering steps\"\"\"\n",
    "    # Create copy of input dataframe\n",
    "    df = loitering_df.copy()\n",
    "\n",
    "    # Ensure datetime columns are in UTC\n",
    "    if df['event_start'].dt.tz is None:\n",
    "        df['event_start'] = df['event_start'].dt.tz_localize('UTC')\n",
    "    if df['event_end'].dt.tz is None:\n",
    "        df['event_end'] = df['event_end'].dt.tz_localize('UTC')\n",
    "\n",
    "    # Add port location features\n",
    "    df = add_port_location_features(df, port_locations_df)\n",
    "\n",
    "    # Create target variable\n",
    "    df = create_target_variable(df)\n",
    "\n",
    "    # Classify loitering location\n",
    "    df = classify_loitering_location(df)\n",
    "\n",
    "    # Calculate distance features\n",
    "    df = calculate_distances(df)\n",
    "\n",
    "    # Extract temporal features\n",
    "    df = extract_temporal_features(df)\n",
    "\n",
    "    # Calculate historical features\n",
    "    df = calculate_historical_features(df)\n",
    "\n",
    "    # Calculate averages and statistics\n",
    "    df = calculate_averages_and_stats(df)\n",
    "\n",
    "    # Calculate port congestion\n",
    "    df = calculate_port_congestion(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "loitering_w_features_df = process_features(loitering_df, port_locations_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Loading - Weather\n",
    "Pulling weather data is done after loitering feature engineering to ensure we only pull relevant days and location of weather need."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get unique date and port location combinations\n",
    "unique_date_ports = loitering_w_features_df[['event_date', 'destination_port_lat', 'destination_port_lon']].drop_duplicates()\n",
    "\n",
    "unique_date_ports = unique_date_ports.sample(5)  # Make sample for testing\n",
    "\n",
    "# Convert event_date to the required format for the API\n",
    "unique_date_ports[\"event_date\"] = pd.to_datetime(unique_date_ports[\"event_date\"]).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# WeatherAPI Key\n",
    "# API_KEY = \"e3596b537e82495c9e615821242411\"\n",
    "\n",
    "# Base URL for the WeatherAPI History API\n",
    "BASE_URL = \"http://api.weatherapi.com/v1/history.json\"\n",
    "\n",
    "# Function to fetch weather data\n",
    "def fetch_weather_data(lat, lon, date, api_key):\n",
    "    try:\n",
    "        # Construct the API request URL\n",
    "        url = f\"{BASE_URL}?key={api_key}&q={lat},{lon}&dt={date}\"\n",
    "\n",
    "        # Make the API call\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the JSON response\n",
    "        weather_data = response.json()\n",
    "        return weather_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {lat}, {lon} on {date}: {e}\")\n",
    "        return None\n",
    "\n",
    "# List to store the results\n",
    "weather_results = []\n",
    "\n",
    "# Iterate through the DataFrame rows\n",
    "for index, row in unique_date_ports.iterrows():\n",
    "    lat = row['destination_port_lat']\n",
    "    lon = row['destination_port_lon']\n",
    "    date = row['event_date']\n",
    "\n",
    "    # Fetch weather data for the given row\n",
    "    weather_data = fetch_weather_data(lat, lon, date, API_KEY)\n",
    "    if weather_data:\n",
    "        # Extract relevant fields from the 'forecastday' object\n",
    "        forecast_day = weather_data.get('forecast', {}).get('forecastday', [{}])[0]\n",
    "        day_data = forecast_day.get('day', {})\n",
    "        day_data['latitude'] = lat\n",
    "        day_data['longitude'] = lon\n",
    "        day_data['date'] = date\n",
    "\n",
    "        # Append the extracted data to the results\n",
    "        weather_results.append(day_data)\n",
    "\n",
    "# Convert results into a DataFrame\n",
    "weather_df = pd.DataFrame(weather_results)\n",
    "\n",
    "# Select only the required columns\n",
    "required_columns = [\n",
    "    \"avgtemp_c\", \"maxwind_kph\", \"totalprecip_mm\", \"totalsnow_cm\", \"avgvis_km\",\n",
    "    \"daily_chance_of_rain\", \"daily_chance_of_snow\", \"latitude\", \"longitude\", \"date\"\n",
    "]\n",
    "weather_df = weather_df[required_columns]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Dropping irrelevant columns\n",
    "Some columns that were needed for data processing can be dropped, such as: ID/name columns, lat/lon columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "final_df = loitering_weather_df.drop([\n",
    "    'event_id', 'event_type', 'vessel_id', 'event_start', 'event_end', 'lat_mean', 'lon_mean',\n",
    "    'event_info_origin_port.port_id', 'event_info_origin_port.label',\n",
    "    'event_info_destination_port.port_id', 'event_info_destination_port.label',\n",
    "    'origin_port_port_id', 'origin_port_lat','origin_port_lon',\n",
    "    'destination_port_port_id', 'destination_port_lat', 'destination_port_lon'\n",
    "    ],\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# EDA Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 20 nautical miles = 37,040 meters\n",
    "near_port = loitering_df[loitering_df['event_info_distance_from_shore_m'] < 50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "near_port[near_port['event_info_loitering_hours'] > 24].shape[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}